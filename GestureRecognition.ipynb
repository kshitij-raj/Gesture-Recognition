{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "- Shubhi Shukla\n",
    "- Kshitij Raj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:53:58.910332Z",
     "iopub.status.busy": "2021-10-26T12:53:58.910092Z",
     "iopub.status.idle": "2021-10-26T12:54:03.397890Z",
     "shell.execute_reply": "2021-10-26T12:54:03.397084Z",
     "shell.execute_reply.started": "2021-10-26T12:53:58.910306Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:03.400867Z",
     "iopub.status.busy": "2021-10-26T12:54:03.400364Z",
     "iopub.status.idle": "2021-10-26T12:54:03.953445Z",
     "shell.execute_reply": "2021-10-26T12:54:03.952688Z",
     "shell.execute_reply.started": "2021-10-26T12:54:03.400828Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:03.956784Z",
     "iopub.status.busy": "2021-10-26T12:54:03.956558Z",
     "iopub.status.idle": "2021-10-26T12:54:03.985369Z",
     "shell.execute_reply": "2021-10-26T12:54:03.984650Z",
     "shell.execute_reply.started": "2021-10-26T12:54:03.956736Z"
    }
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/home/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/home/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:03.988035Z",
     "iopub.status.busy": "2021-10-26T12:54:03.987614Z",
     "iopub.status.idle": "2021-10-26T12:54:04.011523Z",
     "shell.execute_reply": "2021-10-26T12:54:04.010583Z",
     "shell.execute_reply.started": "2021-10-26T12:54:03.987999Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =  [x for x in range(0,30)]  \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list) // batch_size    \n",
    "        for batch in range(num_batches):     \n",
    "            batch_data = np.zeros((batch_size,len(img_idx),120,120,3))   \n",
    "            batch_labels = np.zeros((batch_size,5))                                \n",
    "            for folder in range(batch_size):                                         \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "                for idx,item in enumerate(img_idx):         \n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image= resize(image,[120,120])\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, : , 0] - np.percentile(image[:, : , 0],5))/ (np.percentile(image[:, : , 0],95) - np.percentile(image[:, : , 0],5))\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, : , 1] - np.percentile(image[:, : , 1],5))/ (np.percentile(image[:, : , 1],95) - np.percentile(image[:, : , 1],5))\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, : , 2] - np.percentile(image[:, : , 2],5))/ (np.percentile(image[:, : , 2],95) - np.percentile(image[:, : , 2],5))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels \n",
    "\n",
    "        \n",
    "        if (len(folder_list))> batch_size*num_batches:\n",
    "            batch=(len(folder_list) // batch_size)\n",
    "            remaining_folders = len(folder_list)- ((len(folder_list) // batch_size)*batch_size)   \n",
    "            batch_data = np.zeros((remaining_folders,len(img_idx),120,120,3)) \n",
    "            batch_labels = np.zeros((remaining_folders,5)) \n",
    "            for folder in range(remaining_folders): \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "                for idx,item in enumerate(img_idx): \n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image= resize(image,[120,120])\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, : , 0] - np.percentile(image[:, : , 0],5))/ (np.percentile(image[:, : , 0],95) - np.percentile(image[:, : , 0],5))\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, : , 1] - np.percentile(image[:, : , 1],5))/ (np.percentile(image[:, : , 1],95) - np.percentile(image[:, : , 1],5))\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, : , 2] - np.percentile(image[:, : , 2],5))/ (np.percentile(image[:, : , 2],95) - np.percentile(image[:, : , 2],5))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:04.013245Z",
     "iopub.status.busy": "2021-10-26T12:54:04.012927Z",
     "iopub.status.idle": "2021-10-26T12:54:04.026406Z",
     "shell.execute_reply": "2021-10-26T12:54:04.025546Z",
     "shell.execute_reply.started": "2021-10-26T12:54:04.013167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/home/datasets/Project_data/train'\n",
    "val_path = '/home/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:04.028227Z",
     "iopub.status.busy": "2021-10-26T12:54:04.027785Z",
     "iopub.status.idle": "2021-10-26T12:54:04.334557Z",
     "shell.execute_reply": "2021-10-26T12:54:04.333722Z",
     "shell.execute_reply.started": "2021-10-26T12:54:04.028132Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "#from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:04.489756Z",
     "iopub.status.busy": "2021-10-26T06:20:04.489500Z",
     "iopub.status.idle": "2021-10-26T06:20:06.878884Z",
     "shell.execute_reply": "2021-10-26T06:20:06.877264Z",
     "shell.execute_reply.started": "2021-10-26T06:20:04.489727Z"
    }
   },
   "outputs": [],
   "source": [
    "Input_shape = (30, 120, 120, 3)\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (2,2,2),input_shape=Input_shape,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, (2,2,2),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:07.757837Z",
     "iopub.status.busy": "2021-10-26T06:20:07.757588Z",
     "iopub.status.idle": "2021-10-26T06:20:07.929675Z",
     "shell.execute_reply": "2021-10-26T06:20:07.929025Z",
     "shell.execute_reply.started": "2021-10-26T06:20:07.757808Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 29, 119, 119, 32)  800       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 29, 119, 119, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 13, 58, 58, 64)    16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 58, 58, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 6, 29, 29, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 29, 29, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 322944)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               41336960  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 41,388,901\n",
      "Trainable params: 41,388,709\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:54:04.337906Z",
     "iopub.status.busy": "2021-10-26T12:54:04.336621Z",
     "iopub.status.idle": "2021-10-26T12:54:04.342418Z",
     "shell.execute_reply": "2021-10-26T12:54:04.341730Z",
     "shell.execute_reply.started": "2021-10-26T12:54:04.337850Z"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:37.688401Z",
     "iopub.status.busy": "2021-10-26T06:20:37.687922Z",
     "iopub.status.idle": "2021-10-26T06:20:37.695628Z",
     "shell.execute_reply": "2021-10-26T06:20:37.694767Z",
     "shell.execute_reply.started": "2021-10-26T06:20:37.688365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:44.088090Z",
     "iopub.status.busy": "2021-10-26T06:20:44.087817Z",
     "iopub.status.idle": "2021-10-26T06:20:44.092783Z",
     "shell.execute_reply": "2021-10-26T06:20:44.091957Z",
     "shell.execute_reply.started": "2021-10-26T06:20:44.088061Z"
    }
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 120, 120, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T06:20:49.086027Z",
     "iopub.status.busy": "2021-10-26T06:20:49.085740Z",
     "iopub.status.idle": "2021-10-26T08:41:00.616668Z",
     "shell.execute_reply": "2021-10-26T08:41:00.615913Z",
     "shell.execute_reply.started": "2021-10-26T06:20:49.085973Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-1360ade66242>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 48.6160 - categorical_accuracy: 0.2172Source path =  /home/datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2614_54_40.749188/model-00001-48.61595-0.21719-72.57760-0.18000.h5\n",
      "83/83 [==============================] - 220s 3s/step - loss: 48.6160 - categorical_accuracy: 0.2172 - val_loss: 72.5776 - val_categorical_accuracy: 0.1800\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.6082 - categorical_accuracy: 0.2323\n",
      "Epoch 00002: saving model to model_init_2021-10-2614_54_40.749188/model-00002-4.60823-0.23228-112.04321-0.17000.h5\n",
      "83/83 [==============================] - 222s 3s/step - loss: 4.6082 - categorical_accuracy: 0.2323 - val_loss: 112.0432 - val_categorical_accuracy: 0.1700\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9538 - categorical_accuracy: 0.2051\n",
      "Epoch 00003: saving model to model_init_2021-10-2614_54_40.749188/model-00003-1.95383-0.20513-96.65238-0.19000.h5\n",
      "83/83 [==============================] - 223s 3s/step - loss: 1.9538 - categorical_accuracy: 0.2051 - val_loss: 96.6524 - val_categorical_accuracy: 0.1900\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6035 - categorical_accuracy: 0.2142\n",
      "Epoch 00004: saving model to model_init_2021-10-2614_54_40.749188/model-00004-1.60350-0.21418-64.16114-0.27000.h5\n",
      "83/83 [==============================] - 227s 3s/step - loss: 1.6035 - categorical_accuracy: 0.2142 - val_loss: 64.1611 - val_categorical_accuracy: 0.2700\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.9958 - categorical_accuracy: 0.1946\n",
      "Epoch 00005: saving model to model_init_2021-10-2614_54_40.749188/model-00005-1.99576-0.19457-26.71898-0.31000.h5\n",
      "83/83 [==============================] - 221s 3s/step - loss: 1.9958 - categorical_accuracy: 0.1946 - val_loss: 26.7190 - val_categorical_accuracy: 0.3100\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6836 - categorical_accuracy: 0.1870\n",
      "Epoch 00006: saving model to model_init_2021-10-2614_54_40.749188/model-00006-1.68357-0.18703-13.97808-0.21000.h5\n",
      "83/83 [==============================] - 225s 3s/step - loss: 1.6836 - categorical_accuracy: 0.1870 - val_loss: 13.9781 - val_categorical_accuracy: 0.2100\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6379 - categorical_accuracy: 0.2232\n",
      "Epoch 00007: saving model to model_init_2021-10-2614_54_40.749188/model-00007-1.63787-0.22323-1.72233-0.21000.h5\n",
      "83/83 [==============================] - 220s 3s/step - loss: 1.6379 - categorical_accuracy: 0.2232 - val_loss: 1.7223 - val_categorical_accuracy: 0.2100\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6012 - categorical_accuracy: 0.2021\n",
      "Epoch 00008: saving model to model_init_2021-10-2614_54_40.749188/model-00008-1.60125-0.20211-1.94028-0.20000.h5\n",
      "83/83 [==============================] - 221s 3s/step - loss: 1.6012 - categorical_accuracy: 0.2021 - val_loss: 1.9403 - val_categorical_accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6036 - categorical_accuracy: 0.2232\n",
      "Epoch 00009: saving model to model_init_2021-10-2614_54_40.749188/model-00009-1.60363-0.22323-2.05276-0.23000.h5\n",
      "83/83 [==============================] - 244s 3s/step - loss: 1.6036 - categorical_accuracy: 0.2232 - val_loss: 2.0528 - val_categorical_accuracy: 0.2300\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6094 - categorical_accuracy: 0.1961\n",
      "Epoch 00010: saving model to model_init_2021-10-2614_54_40.749188/model-00010-1.60941-0.19608-1.70105-0.19000.h5\n",
      "83/83 [==============================] - 232s 3s/step - loss: 1.6094 - categorical_accuracy: 0.1961 - val_loss: 1.7010 - val_categorical_accuracy: 0.1900\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6066 - categorical_accuracy: 0.1991\n",
      "Epoch 00011: saving model to model_init_2021-10-2614_54_40.749188/model-00011-1.60660-0.19910-1.86150-0.19000.h5\n",
      "83/83 [==============================] - 227s 3s/step - loss: 1.6066 - categorical_accuracy: 0.1991 - val_loss: 1.8615 - val_categorical_accuracy: 0.1900\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6065 - categorical_accuracy: 0.2066\n",
      "Epoch 00012: saving model to model_init_2021-10-2614_54_40.749188/model-00012-1.60651-0.20664-1.78406-0.23000.h5\n",
      "83/83 [==============================] - 231s 3s/step - loss: 1.6065 - categorical_accuracy: 0.2066 - val_loss: 1.7841 - val_categorical_accuracy: 0.2300\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6261 - categorical_accuracy: 0.1961\n",
      "Epoch 00013: saving model to model_init_2021-10-2614_54_40.749188/model-00013-1.62611-0.19608-2.94543-0.16000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "83/83 [==============================] - 221s 3s/step - loss: 1.6261 - categorical_accuracy: 0.1961 - val_loss: 2.9454 - val_categorical_accuracy: 0.1600\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6044 - categorical_accuracy: 0.1976\n",
      "Epoch 00014: saving model to model_init_2021-10-2614_54_40.749188/model-00014-1.60443-0.19759-2.06214-0.22000.h5\n",
      "83/83 [==============================] - 207s 2s/step - loss: 1.6044 - categorical_accuracy: 0.1976 - val_loss: 2.0621 - val_categorical_accuracy: 0.2200\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6062 - categorical_accuracy: 0.1946\n",
      "Epoch 00015: saving model to model_init_2021-10-2614_54_40.749188/model-00015-1.60617-0.19457-1.96208-0.20000.h5\n",
      "83/83 [==============================] - 204s 2s/step - loss: 1.6062 - categorical_accuracy: 0.1946 - val_loss: 1.9621 - val_categorical_accuracy: 0.2000\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6058 - categorical_accuracy: 0.1991\n",
      "Epoch 00016: saving model to model_init_2021-10-2614_54_40.749188/model-00016-1.60580-0.19910-1.90304-0.22000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "83/83 [==============================] - 204s 2s/step - loss: 1.6058 - categorical_accuracy: 0.1991 - val_loss: 1.9030 - val_categorical_accuracy: 0.2200\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6032 - categorical_accuracy: 0.2172\n",
      "Epoch 00017: saving model to model_init_2021-10-2614_54_40.749188/model-00017-1.60324-0.21719-2.00948-0.21000.h5\n",
      "83/83 [==============================] - 202s 2s/step - loss: 1.6032 - categorical_accuracy: 0.2172 - val_loss: 2.0095 - val_categorical_accuracy: 0.2100\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6042 - categorical_accuracy: 0.2127\n",
      "Epoch 00018: saving model to model_init_2021-10-2614_54_40.749188/model-00018-1.60421-0.21267-1.74587-0.20000.h5\n",
      "83/83 [==============================] - 206s 2s/step - loss: 1.6042 - categorical_accuracy: 0.2127 - val_loss: 1.7459 - val_categorical_accuracy: 0.2000\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6041 - categorical_accuracy: 0.2081\n",
      "Epoch 00019: saving model to model_init_2021-10-2614_54_40.749188/model-00019-1.60412-0.20814-2.01726-0.22000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "83/83 [==============================] - 208s 3s/step - loss: 1.6041 - categorical_accuracy: 0.2081 - val_loss: 2.0173 - val_categorical_accuracy: 0.2200\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.6016 - categorical_accuracy: 0.2036\n",
      "Epoch 00020: saving model to model_init_2021-10-2614_54_40.749188/model-00020-1.60155-0.20362-1.75056-0.25000.h5\n",
      "83/83 [==============================] - 201s 2s/step - loss: 1.6016 - categorical_accuracy: 0.2036 - val_loss: 1.7506 - val_categorical_accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff948051a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T09:13:17.075906Z",
     "iopub.status.busy": "2021-10-26T09:13:17.075632Z",
     "iopub.status.idle": "2021-10-26T09:13:19.716373Z",
     "shell.execute_reply": "2021-10-26T09:13:19.715633Z",
     "shell.execute_reply.started": "2021-10-26T09:13:17.075876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_2 (Conv3D)            (None, 29, 119, 119, 32)  800       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 119, 119, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 13, 58, 58, 64)    16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 58, 58, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 6, 29, 29, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 29, 29, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 322944)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               41336960  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 41,388,901\n",
      "Trainable params: 41,388,709\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "Input_shape = (30, 120, 120, 3)\n",
    "model2 = Sequential()\n",
    "model2.add(Conv3D(32, (2,2,2),input_shape=Input_shape,activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv3D(64, (2,2,2),activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dense(128,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "model2.add(Dense(256,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model2.add(Dense(5))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T09:15:12.054745Z",
     "iopub.status.busy": "2021-10-26T09:15:12.054486Z",
     "iopub.status.idle": "2021-10-26T09:15:12.063596Z",
     "shell.execute_reply": "2021-10-26T09:15:12.062857Z",
     "shell.execute_reply.started": "2021-10-26T09:15:12.054716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T09:15:13.621149Z",
     "iopub.status.busy": "2021-10-26T09:15:13.620501Z",
     "iopub.status.idle": "2021-10-26T11:42:19.083197Z",
     "shell.execute_reply": "2021-10-26T11:42:19.082088Z",
     "shell.execute_reply.started": "2021-10-26T09:15:13.621110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 16\n",
      "Epoch 1/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 29.7599 - categorical_accuracy: 0.2338Source path =  /home/datasets/Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2614_54_40.749188/model-00001-29.75992-0.23379-3.47802-0.27000.h5\n",
      "42/42 [==============================] - 202s 5s/step - loss: 29.7599 - categorical_accuracy: 0.2338 - val_loss: 3.4780 - val_categorical_accuracy: 0.2700\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.8755 - categorical_accuracy: 0.2398\n",
      "Epoch 00002: saving model to model_init_2021-10-2614_54_40.749188/model-00002-2.87548-0.23982-73.13662-0.23000.h5\n",
      "42/42 [==============================] - 201s 5s/step - loss: 2.8755 - categorical_accuracy: 0.2398 - val_loss: 73.1366 - val_categorical_accuracy: 0.2300\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.0097 - categorical_accuracy: 0.2338\n",
      "Epoch 00003: saving model to model_init_2021-10-2614_54_40.749188/model-00003-2.00971-0.23379-136.95921-0.21000.h5\n",
      "42/42 [==============================] - 201s 5s/step - loss: 2.0097 - categorical_accuracy: 0.2338 - val_loss: 136.9592 - val_categorical_accuracy: 0.2100\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.8344 - categorical_accuracy: 0.2172\n",
      "Epoch 00004: saving model to model_init_2021-10-2614_54_40.749188/model-00004-1.83437-0.21719-152.30823-0.24000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "42/42 [==============================] - 198s 5s/step - loss: 1.8344 - categorical_accuracy: 0.2172 - val_loss: 152.3082 - val_categorical_accuracy: 0.2400\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6203 - categorical_accuracy: 0.2172\n",
      "Epoch 00005: saving model to model_init_2021-10-2614_54_40.749188/model-00005-1.62028-0.21719-182.03333-0.19000.h5\n",
      "42/42 [==============================] - 209s 5s/step - loss: 1.6203 - categorical_accuracy: 0.2172 - val_loss: 182.0333 - val_categorical_accuracy: 0.1900\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6805 - categorical_accuracy: 0.2278\n",
      "Epoch 00006: saving model to model_init_2021-10-2614_54_40.749188/model-00006-1.68053-0.22775-188.64465-0.21000.h5\n",
      "42/42 [==============================] - 210s 5s/step - loss: 1.6805 - categorical_accuracy: 0.2278 - val_loss: 188.6447 - val_categorical_accuracy: 0.2100\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7580 - categorical_accuracy: 0.2187\n",
      "Epoch 00007: saving model to model_init_2021-10-2614_54_40.749188/model-00007-1.75805-0.21870-157.70984-0.22000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "42/42 [==============================] - 212s 5s/step - loss: 1.7580 - categorical_accuracy: 0.2187 - val_loss: 157.7098 - val_categorical_accuracy: 0.2200\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5983 - categorical_accuracy: 0.2142\n",
      "Epoch 00008: saving model to model_init_2021-10-2614_54_40.749188/model-00008-1.59833-0.21418-116.02012-0.23000.h5\n",
      "42/42 [==============================] - 224s 5s/step - loss: 1.5983 - categorical_accuracy: 0.2142 - val_loss: 116.0201 - val_categorical_accuracy: 0.2300\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6008 - categorical_accuracy: 0.2112\n",
      "Epoch 00009: saving model to model_init_2021-10-2614_54_40.749188/model-00009-1.60080-0.21116-72.42699-0.27000.h5\n",
      "42/42 [==============================] - 220s 5s/step - loss: 1.6008 - categorical_accuracy: 0.2112 - val_loss: 72.4270 - val_categorical_accuracy: 0.2700\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7427 - categorical_accuracy: 0.2112\n",
      "Epoch 00010: saving model to model_init_2021-10-2614_54_40.749188/model-00010-1.74274-0.21116-54.64407-0.20000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "42/42 [==============================] - 219s 5s/step - loss: 1.7427 - categorical_accuracy: 0.2112 - val_loss: 54.6441 - val_categorical_accuracy: 0.2000\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6657 - categorical_accuracy: 0.2097\n",
      "Epoch 00011: saving model to model_init_2021-10-2614_54_40.749188/model-00011-1.66570-0.20965-20.86321-0.33000.h5\n",
      "42/42 [==============================] - 221s 5s/step - loss: 1.6657 - categorical_accuracy: 0.2097 - val_loss: 20.8632 - val_categorical_accuracy: 0.3300\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5980 - categorical_accuracy: 0.2142\n",
      "Epoch 00012: saving model to model_init_2021-10-2614_54_40.749188/model-00012-1.59799-0.21418-14.27415-0.39000.h5\n",
      "42/42 [==============================] - 220s 5s/step - loss: 1.5980 - categorical_accuracy: 0.2142 - val_loss: 14.2742 - val_categorical_accuracy: 0.3900\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5966 - categorical_accuracy: 0.2142\n",
      "Epoch 00013: saving model to model_init_2021-10-2614_54_40.749188/model-00013-1.59659-0.21418-11.35480-0.32000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "42/42 [==============================] - 235s 6s/step - loss: 1.5966 - categorical_accuracy: 0.2142 - val_loss: 11.3548 - val_categorical_accuracy: 0.3200\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6063 - categorical_accuracy: 0.2081\n",
      "Epoch 00014: saving model to model_init_2021-10-2614_54_40.749188/model-00014-1.60630-0.20814-5.32420-0.36000.h5\n",
      "42/42 [==============================] - 233s 6s/step - loss: 1.6063 - categorical_accuracy: 0.2081 - val_loss: 5.3242 - val_categorical_accuracy: 0.3600\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6017 - categorical_accuracy: 0.2112\n",
      "Epoch 00015: saving model to model_init_2021-10-2614_54_40.749188/model-00015-1.60169-0.21116-3.80178-0.34000.h5\n",
      "42/42 [==============================] - 229s 5s/step - loss: 1.6017 - categorical_accuracy: 0.2112 - val_loss: 3.8018 - val_categorical_accuracy: 0.3400\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5990 - categorical_accuracy: 0.2127\n",
      "Epoch 00016: saving model to model_init_2021-10-2614_54_40.749188/model-00016-1.59901-0.21267-3.59944-0.22000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "42/42 [==============================] - 227s 5s/step - loss: 1.5990 - categorical_accuracy: 0.2127 - val_loss: 3.5994 - val_categorical_accuracy: 0.2200\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5940 - categorical_accuracy: 0.2157\n",
      "Epoch 00017: saving model to model_init_2021-10-2614_54_40.749188/model-00017-1.59398-0.21569-2.16919-0.32000.h5\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1.5940 - categorical_accuracy: 0.2157 - val_loss: 2.1692 - val_categorical_accuracy: 0.3200\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5994 - categorical_accuracy: 0.2127\n",
      "Epoch 00018: saving model to model_init_2021-10-2614_54_40.749188/model-00018-1.59939-0.21267-2.25127-0.24000.h5\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1.5994 - categorical_accuracy: 0.2127 - val_loss: 2.2513 - val_categorical_accuracy: 0.2400\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5989 - categorical_accuracy: 0.2112\n",
      "Epoch 00019: saving model to model_init_2021-10-2614_54_40.749188/model-00019-1.59893-0.21116-2.64858-0.31000.h5\n",
      "42/42 [==============================] - 230s 5s/step - loss: 1.5989 - categorical_accuracy: 0.2112 - val_loss: 2.6486 - val_categorical_accuracy: 0.3100\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6045 - categorical_accuracy: 0.2097\n",
      "Epoch 00020: saving model to model_init_2021-10-2614_54_40.749188/model-00020-1.60447-0.20965-2.27447-0.23000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "42/42 [==============================] - 228s 5s/step - loss: 1.6045 - categorical_accuracy: 0.2097 - val_loss: 2.2745 - val_categorical_accuracy: 0.2300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9d77dba58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:55:16.212816Z",
     "iopub.status.busy": "2021-10-26T12:55:16.212065Z",
     "iopub.status.idle": "2021-10-26T12:55:18.925160Z",
     "shell.execute_reply": "2021-10-26T12:55:18.924406Z",
     "shell.execute_reply.started": "2021-10-26T12:55:16.212765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 29, 119, 119, 32)  800       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 29, 119, 119, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 13, 58, 58, 64)    16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 13, 58, 58, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 6, 29, 29, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 6, 29, 29, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 5, 28, 28, 128)    65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 28, 28, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 2, 14, 14, 128)    0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 14, 14, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               12845312  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 12,930,405\n",
      "Trainable params: 12,929,957\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "Input_shape = (30, 120, 120, 3)\n",
    "model3 = Sequential()\n",
    "model3.add(Conv3D(32, (2,2,2),input_shape=Input_shape,activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(64, (2,2,2),activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(128, (2,2,2),activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dense(256,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model3.add(Dense(5))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:55:25.812864Z",
     "iopub.status.busy": "2021-10-26T12:55:25.812478Z",
     "iopub.status.idle": "2021-10-26T12:55:25.821518Z",
     "shell.execute_reply": "2021-10-26T12:55:25.820459Z",
     "shell.execute_reply.started": "2021-10-26T12:55:25.812826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T12:55:56.161458Z",
     "iopub.status.busy": "2021-10-26T12:55:56.161174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 32.8031 - categorical_accuracy: 0.2640Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.29374, saving model to model_init_2021-10-2614_54_40.749188/model-00001-32.80305-0.26395-2.29374-0.30000.h5\n",
      "21/21 [==============================] - 221s 11s/step - loss: 32.8031 - categorical_accuracy: 0.2640 - val_loss: 2.2937 - val_categorical_accuracy: 0.3000\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8860 - categorical_accuracy: 0.3288\n",
      "Epoch 00002: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 213s 10s/step - loss: 1.8860 - categorical_accuracy: 0.3288 - val_loss: 56.2931 - val_categorical_accuracy: 0.3000\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5224 - categorical_accuracy: 0.3560\n",
      "Epoch 00003: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 213s 10s/step - loss: 1.5224 - categorical_accuracy: 0.3560 - val_loss: 180.1424 - val_categorical_accuracy: 0.1900\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3421 - categorical_accuracy: 0.4525\n",
      "Epoch 00004: val_loss did not improve from 2.29374\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "21/21 [==============================] - 207s 10s/step - loss: 1.3421 - categorical_accuracy: 0.4525 - val_loss: 194.8437 - val_categorical_accuracy: 0.2300\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3096 - categorical_accuracy: 0.4299\n",
      "Epoch 00005: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 218s 10s/step - loss: 1.3096 - categorical_accuracy: 0.4299 - val_loss: 193.5575 - val_categorical_accuracy: 0.2300\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2059 - categorical_accuracy: 0.4555\n",
      "Epoch 00006: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 214s 10s/step - loss: 1.2059 - categorical_accuracy: 0.4555 - val_loss: 175.9559 - val_categorical_accuracy: 0.2500\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0849 - categorical_accuracy: 0.5083\n",
      "Epoch 00007: val_loss did not improve from 2.29374\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21/21 [==============================] - 215s 10s/step - loss: 1.0849 - categorical_accuracy: 0.5083 - val_loss: 155.1255 - val_categorical_accuracy: 0.2700\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0298 - categorical_accuracy: 0.5370\n",
      "Epoch 00008: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 209s 10s/step - loss: 1.0298 - categorical_accuracy: 0.5370 - val_loss: 151.3622 - val_categorical_accuracy: 0.2300\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0011 - categorical_accuracy: 0.5490\n",
      "Epoch 00009: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 210s 10s/step - loss: 1.0011 - categorical_accuracy: 0.5490 - val_loss: 139.8268 - val_categorical_accuracy: 0.2300\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0209 - categorical_accuracy: 0.5279\n",
      "Epoch 00010: val_loss did not improve from 2.29374\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21/21 [==============================] - 198s 9s/step - loss: 1.0209 - categorical_accuracy: 0.5279 - val_loss: 123.2394 - val_categorical_accuracy: 0.2500\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9934 - categorical_accuracy: 0.5535\n",
      "Epoch 00011: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 200s 10s/step - loss: 0.9934 - categorical_accuracy: 0.5535 - val_loss: 116.0394 - val_categorical_accuracy: 0.2700\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9311 - categorical_accuracy: 0.5807\n",
      "Epoch 00012: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 190s 9s/step - loss: 0.9311 - categorical_accuracy: 0.5807 - val_loss: 112.7905 - val_categorical_accuracy: 0.2500\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9166 - categorical_accuracy: 0.5520\n",
      "Epoch 00013: val_loss did not improve from 2.29374\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "21/21 [==============================] - 226s 11s/step - loss: 0.9166 - categorical_accuracy: 0.5520 - val_loss: 106.6838 - val_categorical_accuracy: 0.2400\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9372 - categorical_accuracy: 0.5656\n",
      "Epoch 00014: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 215s 10s/step - loss: 0.9372 - categorical_accuracy: 0.5656 - val_loss: 100.2656 - val_categorical_accuracy: 0.2500\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9114 - categorical_accuracy: 0.5611\n",
      "Epoch 00015: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 202s 10s/step - loss: 0.9114 - categorical_accuracy: 0.5611 - val_loss: 96.5536 - val_categorical_accuracy: 0.2700\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9054 - categorical_accuracy: 0.5747\n",
      "Epoch 00016: val_loss did not improve from 2.29374\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "21/21 [==============================] - 197s 9s/step - loss: 0.9054 - categorical_accuracy: 0.5747 - val_loss: 85.5096 - val_categorical_accuracy: 0.2700\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9150 - categorical_accuracy: 0.5852\n",
      "Epoch 00017: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 206s 10s/step - loss: 0.9150 - categorical_accuracy: 0.5852 - val_loss: 78.1387 - val_categorical_accuracy: 0.2500\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8816 - categorical_accuracy: 0.5882\n",
      "Epoch 00018: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 206s 10s/step - loss: 0.8816 - categorical_accuracy: 0.5882 - val_loss: 71.0674 - val_categorical_accuracy: 0.2100\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9112 - categorical_accuracy: 0.5777\n",
      "Epoch 00019: val_loss did not improve from 2.29374\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "21/21 [==============================] - 203s 10s/step - loss: 0.9112 - categorical_accuracy: 0.5777 - val_loss: 60.7507 - val_categorical_accuracy: 0.2900\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9062 - categorical_accuracy: 0.5551\n",
      "Epoch 00020: val_loss did not improve from 2.29374\n",
      "21/21 [==============================] - 199s 9s/step - loss: 0.9062 - categorical_accuracy: 0.5551 - val_loss: 52.0788 - val_categorical_accuracy: 0.2900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff88cee3ac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_26 (Conv3D)           (None, 29, 119, 119, 16)  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 29, 119, 119, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 14, 59, 59, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 14, 59, 59, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 13, 58, 58, 32)    4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 13, 58, 58, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 6, 29, 29, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 6, 29, 29, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 5, 28, 28, 64)     16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 5, 28, 28, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 2, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 2, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 6,445,493\n",
      "Trainable params: 6,445,269\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "Input_shape = (30, 120, 120, 3)\n",
    "model4 = Sequential()\n",
    "model4.add(Conv3D(16, (2,2,2),input_shape=Input_shape,activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Conv3D(32, (2,2,2),activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Conv3D(64, (2,2,2),activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "model4.add(Dense(256,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model4.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model4.add(Dense(5))\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "model4.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-3dfaee9dbea1>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 24.3242 - categorical_accuracy: 0.2293 Source path =  /home/datasets/Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67214, saving model to model_init_2021-10-2704_53_32.764571/model-00001-24.32424-0.22926-1.67214-0.31000.h5\n",
      "11/11 [==============================] - 203s 18s/step - loss: 24.3242 - categorical_accuracy: 0.2293 - val_loss: 1.6721 - val_categorical_accuracy: 0.3100\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.1658 - categorical_accuracy: 0.4314 \n",
      "Epoch 00002: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 202s 18s/step - loss: 3.1658 - categorical_accuracy: 0.4314 - val_loss: 7.4262 - val_categorical_accuracy: 0.2700\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2238 - categorical_accuracy: 0.4796 \n",
      "Epoch 00003: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 207s 19s/step - loss: 1.2238 - categorical_accuracy: 0.4796 - val_loss: 15.7204 - val_categorical_accuracy: 0.2100\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0850 - categorical_accuracy: 0.5505 \n",
      "Epoch 00004: val_loss did not improve from 1.67214\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "11/11 [==============================] - 204s 19s/step - loss: 1.0850 - categorical_accuracy: 0.5505 - val_loss: 21.4955 - val_categorical_accuracy: 0.2200\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9434 - categorical_accuracy: 0.6094 \n",
      "Epoch 00005: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 218s 20s/step - loss: 0.9434 - categorical_accuracy: 0.6094 - val_loss: 25.7304 - val_categorical_accuracy: 0.2100\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8015 - categorical_accuracy: 0.6772 \n",
      "Epoch 00006: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 206s 19s/step - loss: 0.8015 - categorical_accuracy: 0.6772 - val_loss: 28.8202 - val_categorical_accuracy: 0.2200\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7104 - categorical_accuracy: 0.7225 \n",
      "Epoch 00007: val_loss did not improve from 1.67214\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "11/11 [==============================] - 211s 19s/step - loss: 0.7104 - categorical_accuracy: 0.7225 - val_loss: 33.4770 - val_categorical_accuracy: 0.2100\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6475 - categorical_accuracy: 0.7481 \n",
      "Epoch 00008: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 200s 18s/step - loss: 0.6475 - categorical_accuracy: 0.7481 - val_loss: 43.2896 - val_categorical_accuracy: 0.1600\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6042 - categorical_accuracy: 0.7617 \n",
      "Epoch 00009: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 202s 18s/step - loss: 0.6042 - categorical_accuracy: 0.7617 - val_loss: 40.4376 - val_categorical_accuracy: 0.2100\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5158 - categorical_accuracy: 0.7994 \n",
      "Epoch 00010: val_loss did not improve from 1.67214\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "11/11 [==============================] - 193s 18s/step - loss: 0.5158 - categorical_accuracy: 0.7994 - val_loss: 44.2814 - val_categorical_accuracy: 0.2000\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4991 - categorical_accuracy: 0.7888 \n",
      "Epoch 00011: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 201s 18s/step - loss: 0.4991 - categorical_accuracy: 0.7888 - val_loss: 46.9850 - val_categorical_accuracy: 0.2100\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5100 - categorical_accuracy: 0.8175 \n",
      "Epoch 00012: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 193s 18s/step - loss: 0.5100 - categorical_accuracy: 0.8175 - val_loss: 47.2500 - val_categorical_accuracy: 0.2200\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4541 - categorical_accuracy: 0.8235 \n",
      "Epoch 00013: val_loss did not improve from 1.67214\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "11/11 [==============================] - 207s 19s/step - loss: 0.4541 - categorical_accuracy: 0.8235 - val_loss: 52.1118 - val_categorical_accuracy: 0.2100\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4367 - categorical_accuracy: 0.8175 \n",
      "Epoch 00014: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 204s 19s/step - loss: 0.4367 - categorical_accuracy: 0.8175 - val_loss: 49.3176 - val_categorical_accuracy: 0.2500\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4613 - categorical_accuracy: 0.8250 \n",
      "Epoch 00015: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 215s 20s/step - loss: 0.4613 - categorical_accuracy: 0.8250 - val_loss: 55.8420 - val_categorical_accuracy: 0.2100\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4260 - categorical_accuracy: 0.8326 \n",
      "Epoch 00016: val_loss did not improve from 1.67214\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "11/11 [==============================] - 203s 18s/step - loss: 0.4260 - categorical_accuracy: 0.8326 - val_loss: 62.4880 - val_categorical_accuracy: 0.1700\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4344 - categorical_accuracy: 0.8205 \n",
      "Epoch 00017: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 210s 19s/step - loss: 0.4344 - categorical_accuracy: 0.8205 - val_loss: 58.8129 - val_categorical_accuracy: 0.2100\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4167 - categorical_accuracy: 0.8446 \n",
      "Epoch 00018: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 194s 18s/step - loss: 0.4167 - categorical_accuracy: 0.8446 - val_loss: 55.2530 - val_categorical_accuracy: 0.2400\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4331 - categorical_accuracy: 0.8356 \n",
      "Epoch 00019: val_loss did not improve from 1.67214\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "11/11 [==============================] - 204s 19s/step - loss: 0.4331 - categorical_accuracy: 0.8356 - val_loss: 60.7183 - val_categorical_accuracy: 0.2100\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4277 - categorical_accuracy: 0.8462 \n",
      "Epoch 00020: val_loss did not improve from 1.67214\n",
      "11/11 [==============================] - 204s 19s/step - loss: 0.4277 - categorical_accuracy: 0.8462 - val_loss: 57.2308 - val_categorical_accuracy: 0.2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbdbc584b38>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_29 (Conv3D)           (None, 29, 119, 119, 16)  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 29, 119, 119, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_29 (MaxPooling (None, 14, 59, 59, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 14, 59, 59, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 13, 58, 58, 32)    4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 13, 58, 58, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling (None, 6, 29, 29, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 6, 29, 29, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 5, 28, 28, 64)     16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 5, 28, 28, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling (None, 2, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 2, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 12,869,557\n",
      "Trainable params: 12,869,333\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "Input_shape = (30, 120, 120, 3)\n",
    "model5 = Sequential()\n",
    "model5.add(Conv3D(16, (2,2,2),input_shape=Input_shape,activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv3D(32, (2,2,2),activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv3D(64, (2,2,2),activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Flatten())\n",
    "\n",
    "model5.add(Dense(512,activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model5.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model5.add(Dense(5))\n",
    "model5.add(Activation('softmax'))\n",
    "\n",
    "model5.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 29.2400 - categorical_accuracy: 0.2971Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.01275, saving model to model_init_2021-10-2704_53_32.764571/model-00001-29.24002-0.29713-3.01275-0.37000.h5\n",
      "21/21 [==============================] - 207s 10s/step - loss: 29.2400 - categorical_accuracy: 0.2971 - val_loss: 3.0127 - val_categorical_accuracy: 0.3700\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.9825 - categorical_accuracy: 0.4676\n",
      "Epoch 00002: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 204s 10s/step - loss: 3.9825 - categorical_accuracy: 0.4676 - val_loss: 8.7694 - val_categorical_accuracy: 0.2200\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3673 - categorical_accuracy: 0.5415\n",
      "Epoch 00003: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 209s 10s/step - loss: 1.3673 - categorical_accuracy: 0.5415 - val_loss: 59.5607 - val_categorical_accuracy: 0.2200\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0891 - categorical_accuracy: 0.5867\n",
      "Epoch 00004: val_loss did not improve from 3.01275\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "21/21 [==============================] - 205s 10s/step - loss: 1.0891 - categorical_accuracy: 0.5867 - val_loss: 92.3854 - val_categorical_accuracy: 0.2200\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7797 - categorical_accuracy: 0.6953\n",
      "Epoch 00005: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 222s 11s/step - loss: 0.7797 - categorical_accuracy: 0.6953 - val_loss: 112.0809 - val_categorical_accuracy: 0.2100\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7333 - categorical_accuracy: 0.7376\n",
      "Epoch 00006: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 222s 11s/step - loss: 0.7333 - categorical_accuracy: 0.7376 - val_loss: 117.2864 - val_categorical_accuracy: 0.2500\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5374 - categorical_accuracy: 0.7873\n",
      "Epoch 00007: val_loss did not improve from 3.01275\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21/21 [==============================] - 222s 11s/step - loss: 0.5374 - categorical_accuracy: 0.7873 - val_loss: 134.4328 - val_categorical_accuracy: 0.1700\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4721 - categorical_accuracy: 0.8296\n",
      "Epoch 00008: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 211s 10s/step - loss: 0.4721 - categorical_accuracy: 0.8296 - val_loss: 129.4448 - val_categorical_accuracy: 0.2200\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4215 - categorical_accuracy: 0.8507\n",
      "Epoch 00009: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 223s 11s/step - loss: 0.4215 - categorical_accuracy: 0.8507 - val_loss: 131.1950 - val_categorical_accuracy: 0.2100\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3706 - categorical_accuracy: 0.8718\n",
      "Epoch 00010: val_loss did not improve from 3.01275\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21/21 [==============================] - 221s 11s/step - loss: 0.3706 - categorical_accuracy: 0.8718 - val_loss: 124.9490 - val_categorical_accuracy: 0.2200\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3228 - categorical_accuracy: 0.8884\n",
      "Epoch 00011: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 212s 10s/step - loss: 0.3228 - categorical_accuracy: 0.8884 - val_loss: 127.2116 - val_categorical_accuracy: 0.1900\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3110 - categorical_accuracy: 0.8899\n",
      "Epoch 00012: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 203s 10s/step - loss: 0.3110 - categorical_accuracy: 0.8899 - val_loss: 118.0326 - val_categorical_accuracy: 0.2200\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2718 - categorical_accuracy: 0.9095\n",
      "Epoch 00013: val_loss did not improve from 3.01275\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "21/21 [==============================] - 218s 10s/step - loss: 0.2718 - categorical_accuracy: 0.9095 - val_loss: 113.5487 - val_categorical_accuracy: 0.2100\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2594 - categorical_accuracy: 0.9231\n",
      "Epoch 00014: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 215s 10s/step - loss: 0.2594 - categorical_accuracy: 0.9231 - val_loss: 109.1015 - val_categorical_accuracy: 0.2200\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2825 - categorical_accuracy: 0.9140\n",
      "Epoch 00015: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 217s 10s/step - loss: 0.2825 - categorical_accuracy: 0.9140 - val_loss: 98.9495 - val_categorical_accuracy: 0.2100\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2571 - categorical_accuracy: 0.9216\n",
      "Epoch 00016: val_loss did not improve from 3.01275\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "21/21 [==============================] - 210s 10s/step - loss: 0.2571 - categorical_accuracy: 0.9216 - val_loss: 89.6741 - val_categorical_accuracy: 0.2000\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2249 - categorical_accuracy: 0.9321\n",
      "Epoch 00017: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 213s 10s/step - loss: 0.2249 - categorical_accuracy: 0.9321 - val_loss: 81.0100 - val_categorical_accuracy: 0.2100\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2345 - categorical_accuracy: 0.9216\n",
      "Epoch 00018: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 213s 10s/step - loss: 0.2345 - categorical_accuracy: 0.9216 - val_loss: 73.2072 - val_categorical_accuracy: 0.2100\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2171 - categorical_accuracy: 0.9246\n",
      "Epoch 00019: val_loss did not improve from 3.01275\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "21/21 [==============================] - 207s 10s/step - loss: 0.2171 - categorical_accuracy: 0.9246 - val_loss: 61.6301 - val_categorical_accuracy: 0.2400\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2144 - categorical_accuracy: 0.9367\n",
      "Epoch 00020: val_loss did not improve from 3.01275\n",
      "21/21 [==============================] - 207s 10s/step - loss: 0.2144 - categorical_accuracy: 0.9367 - val_loss: 52.6982 - val_categorical_accuracy: 0.2200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbdbc2ae518>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T11:51:59.415259Z",
     "iopub.status.busy": "2021-10-24T11:51:59.414819Z",
     "iopub.status.idle": "2021-10-24T11:51:59.511812Z",
     "shell.execute_reply": "2021-10-24T11:51:59.511152Z",
     "shell.execute_reply.started": "2021-10-24T11:51:59.41522Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16,32,64 kernels in 3 consecutive conv3D layers\n",
    "model6 = Sequential()\n",
    "# 1st Layer Group\n",
    "model6.add(Conv3D(16, kernel_size=(3,3,3), input_shape=(30,120,120,3),padding='same'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "# 2nd Layer Group\n",
    "model6.add(Conv3D(32, kernel_size=(3,3,3),padding='same'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "model6.add(layers.Dropout(0.25))\n",
    "# 3rd Layer Group\n",
    "model6.add(Conv3D(64, kernel_size=(3,3,3),padding='same'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling3D(pool_size=(3,3,3)))\n",
    "\n",
    "# FC layer group\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(500, activation='relu'))\n",
    "model6.add(layers.Dropout(0.5))\n",
    "# Softmax Layer\n",
    "model6.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_32 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_32 (MaxPooling (None, 10, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 10, 40, 40, 32)    13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 10, 40, 40, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_33 (MaxPooling (None, 3, 13, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 3, 13, 13, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 3, 13, 13, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 3, 13, 13, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 13, 13, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_34 (MaxPooling (None, 1, 4, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 585,981\n",
      "Trainable params: 585,757\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model6.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 8\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.8011 - categorical_accuracy: 0.2685Source path =  /home/datasets/Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2619_11_58.637805/model-00001-2.80113-0.26848-1.69528-0.26000.h5\n",
      "83/83 [==============================] - 208s 3s/step - loss: 2.8011 - categorical_accuracy: 0.2685 - val_loss: 1.6953 - val_categorical_accuracy: 0.2600\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.5047 - categorical_accuracy: 0.3514\n",
      "Epoch 00002: saving model to model_init_2021-10-2619_11_58.637805/model-00002-1.50472-0.35143-2.04884-0.21000.h5\n",
      "83/83 [==============================] - 203s 2s/step - loss: 1.5047 - categorical_accuracy: 0.3514 - val_loss: 2.0488 - val_categorical_accuracy: 0.2100\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.3697 - categorical_accuracy: 0.4163\n",
      "Epoch 00003: saving model to model_init_2021-10-2619_11_58.637805/model-00003-1.36973-0.41629-2.60021-0.22000.h5\n",
      "83/83 [==============================] - 201s 2s/step - loss: 1.3697 - categorical_accuracy: 0.4163 - val_loss: 2.6002 - val_categorical_accuracy: 0.2200\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.2148 - categorical_accuracy: 0.4736\n",
      "Epoch 00004: saving model to model_init_2021-10-2619_11_58.637805/model-00004-1.21485-0.47360-3.29352-0.21000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "83/83 [==============================] - 198s 2s/step - loss: 1.2148 - categorical_accuracy: 0.4736 - val_loss: 3.2935 - val_categorical_accuracy: 0.2100\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 1.0038 - categorical_accuracy: 0.6018\n",
      "Epoch 00005: saving model to model_init_2021-10-2619_11_58.637805/model-00005-1.00385-0.60181-2.74124-0.21000.h5\n",
      "83/83 [==============================] - 197s 2s/step - loss: 1.0038 - categorical_accuracy: 0.6018 - val_loss: 2.7412 - val_categorical_accuracy: 0.2100\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.8872 - categorical_accuracy: 0.6576\n",
      "Epoch 00006: saving model to model_init_2021-10-2619_11_58.637805/model-00006-0.88721-0.65762-1.66271-0.34000.h5\n",
      "83/83 [==============================] - 197s 2s/step - loss: 0.8872 - categorical_accuracy: 0.6576 - val_loss: 1.6627 - val_categorical_accuracy: 0.3400\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.7612 - categorical_accuracy: 0.7119\n",
      "Epoch 00007: saving model to model_init_2021-10-2619_11_58.637805/model-00007-0.76120-0.71192-0.95294-0.65000.h5\n",
      "83/83 [==============================] - 199s 2s/step - loss: 0.7612 - categorical_accuracy: 0.7119 - val_loss: 0.9529 - val_categorical_accuracy: 0.6500\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.6408 - categorical_accuracy: 0.7783\n",
      "Epoch 00008: saving model to model_init_2021-10-2619_11_58.637805/model-00008-0.64084-0.77828-0.86468-0.67000.h5\n",
      "83/83 [==============================] - 201s 2s/step - loss: 0.6408 - categorical_accuracy: 0.7783 - val_loss: 0.8647 - val_categorical_accuracy: 0.6700\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.5473 - categorical_accuracy: 0.7964\n",
      "Epoch 00009: saving model to model_init_2021-10-2619_11_58.637805/model-00009-0.54732-0.79638-0.97324-0.60000.h5\n",
      "83/83 [==============================] - 199s 2s/step - loss: 0.5473 - categorical_accuracy: 0.7964 - val_loss: 0.9732 - val_categorical_accuracy: 0.6000\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4513 - categorical_accuracy: 0.8507\n",
      "Epoch 00010: saving model to model_init_2021-10-2619_11_58.637805/model-00010-0.45132-0.85068-1.08449-0.55000.h5\n",
      "83/83 [==============================] - 199s 2s/step - loss: 0.4513 - categorical_accuracy: 0.8507 - val_loss: 1.0845 - val_categorical_accuracy: 0.5500\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.4540 - categorical_accuracy: 0.8416\n",
      "Epoch 00011: saving model to model_init_2021-10-2619_11_58.637805/model-00011-0.45402-0.84163-0.35198-0.90000.h5\n",
      "83/83 [==============================] - 200s 2s/step - loss: 0.4540 - categorical_accuracy: 0.8416 - val_loss: 0.3520 - val_categorical_accuracy: 0.9000\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.3831 - categorical_accuracy: 0.8582\n",
      "Epoch 00012: saving model to model_init_2021-10-2619_11_58.637805/model-00012-0.38305-0.85822-0.38632-0.86000.h5\n",
      "83/83 [==============================] - 200s 2s/step - loss: 0.3831 - categorical_accuracy: 0.8582 - val_loss: 0.3863 - val_categorical_accuracy: 0.8600\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2680 - categorical_accuracy: 0.9035\n",
      "Epoch 00013: saving model to model_init_2021-10-2619_11_58.637805/model-00013-0.26803-0.90347-0.64247-0.76000.h5\n",
      "83/83 [==============================] - 199s 2s/step - loss: 0.2680 - categorical_accuracy: 0.9035 - val_loss: 0.6425 - val_categorical_accuracy: 0.7600\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2832 - categorical_accuracy: 0.8944\n",
      "Epoch 00014: saving model to model_init_2021-10-2619_11_58.637805/model-00014-0.28316-0.89442-0.87368-0.70000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "83/83 [==============================] - 201s 2s/step - loss: 0.2832 - categorical_accuracy: 0.8944 - val_loss: 0.8737 - val_categorical_accuracy: 0.7000\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2199 - categorical_accuracy: 0.9186\n",
      "Epoch 00015: saving model to model_init_2021-10-2619_11_58.637805/model-00015-0.21994-0.91855-0.25445-0.90000.h5\n",
      "83/83 [==============================] - 199s 2s/step - loss: 0.2199 - categorical_accuracy: 0.9186 - val_loss: 0.2544 - val_categorical_accuracy: 0.9000\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1899 - categorical_accuracy: 0.9427\n",
      "Epoch 00016: saving model to model_init_2021-10-2619_11_58.637805/model-00016-0.18986-0.94268-0.42960-0.87000.h5\n",
      "83/83 [==============================] - 200s 2s/step - loss: 0.1899 - categorical_accuracy: 0.9427 - val_loss: 0.4296 - val_categorical_accuracy: 0.8700\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1961 - categorical_accuracy: 0.9336\n",
      "Epoch 00017: saving model to model_init_2021-10-2619_11_58.637805/model-00017-0.19607-0.93363-0.41277-0.84000.h5\n",
      "83/83 [==============================] - 198s 2s/step - loss: 0.1961 - categorical_accuracy: 0.9336 - val_loss: 0.4128 - val_categorical_accuracy: 0.8400\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1700 - categorical_accuracy: 0.9306\n",
      "Epoch 00018: saving model to model_init_2021-10-2619_11_58.637805/model-00018-0.16997-0.93062-0.35344-0.89000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "83/83 [==============================] - 200s 2s/step - loss: 0.1700 - categorical_accuracy: 0.9306 - val_loss: 0.3534 - val_categorical_accuracy: 0.8900\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1291 - categorical_accuracy: 0.9638\n",
      "Epoch 00019: saving model to model_init_2021-10-2619_11_58.637805/model-00019-0.12913-0.96380-0.13273-0.94000.h5\n",
      "83/83 [==============================] - 200s 2s/step - loss: 0.1291 - categorical_accuracy: 0.9638 - val_loss: 0.1327 - val_categorical_accuracy: 0.9400\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1339 - categorical_accuracy: 0.9593\n",
      "Epoch 00020: saving model to model_init_2021-10-2619_11_58.637805/model-00020-0.13388-0.95928-0.21135-0.94000.h5\n",
      "83/83 [==============================] - 198s 2s/step - loss: 0.1339 - categorical_accuracy: 0.9593 - val_loss: 0.2114 - val_categorical_accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f55d40b54e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above model has been selected as the best model since it gives the best train and validation accuracy obtained at epoch 18 with 93.9% training accuracy and 89% validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import LSTM, GRU, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7 - Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model7 = Sequential()\n",
    "\n",
    "model7.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(30,120,120,3)))\n",
    "model7.add(TimeDistributed(BatchNormalization()))\n",
    "model7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model7.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model7.add(TimeDistributed(BatchNormalization()))\n",
    "model7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model7.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model7.add(TimeDistributed(BatchNormalization()))\n",
    "model7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model7.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model7.add(TimeDistributed(BatchNormalization()))\n",
    "model7.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "          \n",
    "\n",
    "model7.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model7.add(Dropout(0.25))\n",
    "model7.add(LSTM(64))\n",
    "model7.add(Dropout(0.5))\n",
    "        \n",
    "        \n",
    "model7.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 30, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 30, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 30, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 30, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 30, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 30, 6272)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 6272)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                1622272   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,720,997\n",
      "Trainable params: 1,720,517\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model7.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-a7b728ef5c63>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5895 - categorical_accuracy: 0.2851Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.92079, saving model to model_init_2021-10-2710_41_46.391063/model-00001-1.58954-0.28507-1.92079-0.19000.h5\n",
      "21/21 [==============================] - 211s 10s/step - loss: 1.5895 - categorical_accuracy: 0.2851 - val_loss: 1.9208 - val_categorical_accuracy: 0.1900\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4282 - categorical_accuracy: 0.4012\n",
      "Epoch 00002: val_loss did not improve from 1.92079\n",
      "21/21 [==============================] - 214s 10s/step - loss: 1.4282 - categorical_accuracy: 0.4012 - val_loss: 1.9315 - val_categorical_accuracy: 0.2600\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2801 - categorical_accuracy: 0.5158\n",
      "Epoch 00003: val_loss did not improve from 1.92079\n",
      "21/21 [==============================] - 217s 10s/step - loss: 1.2801 - categorical_accuracy: 0.5158 - val_loss: 2.0269 - val_categorical_accuracy: 0.1700\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1798 - categorical_accuracy: 0.5309\n",
      "Epoch 00004: val_loss did not improve from 1.92079\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "21/21 [==============================] - 216s 10s/step - loss: 1.1798 - categorical_accuracy: 0.5309 - val_loss: 1.9396 - val_categorical_accuracy: 0.2100\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0691 - categorical_accuracy: 0.6214\n",
      "Epoch 00005: val_loss improved from 1.92079 to 1.88550, saving model to model_init_2021-10-2710_41_46.391063/model-00005-1.06908-0.62142-1.88550-0.22000.h5\n",
      "21/21 [==============================] - 226s 11s/step - loss: 1.0691 - categorical_accuracy: 0.6214 - val_loss: 1.8855 - val_categorical_accuracy: 0.2200\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9626 - categorical_accuracy: 0.6652\n",
      "Epoch 00006: val_loss improved from 1.88550 to 1.77097, saving model to model_init_2021-10-2710_41_46.391063/model-00006-0.96256-0.66516-1.77097-0.21000.h5\n",
      "21/21 [==============================] - 230s 11s/step - loss: 0.9626 - categorical_accuracy: 0.6652 - val_loss: 1.7710 - val_categorical_accuracy: 0.2100\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9255 - categorical_accuracy: 0.6863\n",
      "Epoch 00007: val_loss did not improve from 1.77097\n",
      "21/21 [==============================] - 228s 11s/step - loss: 0.9255 - categorical_accuracy: 0.6863 - val_loss: 1.8086 - val_categorical_accuracy: 0.1700\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8269 - categorical_accuracy: 0.7557\n",
      "Epoch 00008: val_loss did not improve from 1.77097\n",
      "21/21 [==============================] - 213s 10s/step - loss: 0.8269 - categorical_accuracy: 0.7557 - val_loss: 1.8073 - val_categorical_accuracy: 0.1700\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7603 - categorical_accuracy: 0.7587\n",
      "Epoch 00009: val_loss improved from 1.77097 to 1.74454, saving model to model_init_2021-10-2710_41_46.391063/model-00009-0.76034-0.75867-1.74454-0.18000.h5\n",
      "21/21 [==============================] - 214s 10s/step - loss: 0.7603 - categorical_accuracy: 0.7587 - val_loss: 1.7445 - val_categorical_accuracy: 0.1800\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6880 - categorical_accuracy: 0.7934\n",
      "Epoch 00010: val_loss did not improve from 1.74454\n",
      "21/21 [==============================] - 213s 10s/step - loss: 0.6880 - categorical_accuracy: 0.7934 - val_loss: 1.8453 - val_categorical_accuracy: 0.2400\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6628 - categorical_accuracy: 0.7813\n",
      "Epoch 00011: val_loss did not improve from 1.74454\n",
      "21/21 [==============================] - 215s 10s/step - loss: 0.6628 - categorical_accuracy: 0.7813 - val_loss: 1.9591 - val_categorical_accuracy: 0.2000\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6072 - categorical_accuracy: 0.8205\n",
      "Epoch 00012: val_loss did not improve from 1.74454\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21/21 [==============================] - 209s 10s/step - loss: 0.6072 - categorical_accuracy: 0.8205 - val_loss: 1.8937 - val_categorical_accuracy: 0.2500\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5222 - categorical_accuracy: 0.8658\n",
      "Epoch 00013: val_loss did not improve from 1.74454\n",
      "21/21 [==============================] - 219s 10s/step - loss: 0.5222 - categorical_accuracy: 0.8658 - val_loss: 1.8852 - val_categorical_accuracy: 0.2300\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4721 - categorical_accuracy: 0.8884\n",
      "Epoch 00014: val_loss did not improve from 1.74454\n",
      "21/21 [==============================] - 217s 10s/step - loss: 0.4721 - categorical_accuracy: 0.8884 - val_loss: 1.7992 - val_categorical_accuracy: 0.1800\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4153 - categorical_accuracy: 0.9095\n",
      "Epoch 00015: val_loss did not improve from 1.74454\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21/21 [==============================] - 219s 10s/step - loss: 0.4153 - categorical_accuracy: 0.9095 - val_loss: 1.9055 - val_categorical_accuracy: 0.2500\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4018 - categorical_accuracy: 0.9080\n",
      "Epoch 00016: val_loss improved from 1.74454 to 1.70271, saving model to model_init_2021-10-2710_41_46.391063/model-00016-0.40176-0.90799-1.70271-0.23000.h5\n",
      "21/21 [==============================] - 208s 10s/step - loss: 0.4018 - categorical_accuracy: 0.9080 - val_loss: 1.7027 - val_categorical_accuracy: 0.2300\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3911 - categorical_accuracy: 0.9201\n",
      "Epoch 00017: val_loss improved from 1.70271 to 1.69265, saving model to model_init_2021-10-2710_41_46.391063/model-00017-0.39110-0.92006-1.69265-0.24000.h5\n",
      "21/21 [==============================] - 215s 10s/step - loss: 0.3911 - categorical_accuracy: 0.9201 - val_loss: 1.6926 - val_categorical_accuracy: 0.2400\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3653 - categorical_accuracy: 0.9276\n",
      "Epoch 00018: val_loss improved from 1.69265 to 1.61591, saving model to model_init_2021-10-2710_41_46.391063/model-00018-0.36527-0.92760-1.61591-0.32000.h5\n",
      "21/21 [==============================] - 216s 10s/step - loss: 0.3653 - categorical_accuracy: 0.9276 - val_loss: 1.6159 - val_categorical_accuracy: 0.3200\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3499 - categorical_accuracy: 0.9397\n",
      "Epoch 00019: val_loss improved from 1.61591 to 1.49426, saving model to model_init_2021-10-2710_41_46.391063/model-00019-0.34992-0.93967-1.49426-0.38000.h5\n",
      "21/21 [==============================] - 218s 10s/step - loss: 0.3499 - categorical_accuracy: 0.9397 - val_loss: 1.4943 - val_categorical_accuracy: 0.3800\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3377 - categorical_accuracy: 0.9487\n",
      "Epoch 00020: val_loss did not improve from 1.49426\n",
      "21/21 [==============================] - 207s 10s/step - loss: 0.3377 - categorical_accuracy: 0.9487 - val_loss: 1.5629 - val_categorical_accuracy: 0.3800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e7859ac88>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8 - Using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =64\n",
    "\n",
    "model8 = Sequential()\n",
    "\n",
    "model8.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(30,120,120,3)))\n",
    "model8.add(TimeDistributed(BatchNormalization()))\n",
    "model8.add(TimeDistributed(MaxPooling2D((3, 3))))\n",
    "        \n",
    "model8.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model8.add(TimeDistributed(BatchNormalization()))\n",
    "model8.add(TimeDistributed(MaxPooling2D((3, 3))))\n",
    "        \n",
    "model8.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model8.add(TimeDistributed(BatchNormalization()))\n",
    "model8.add(TimeDistributed(MaxPooling2D((3, 3))))\n",
    "                 \n",
    "\n",
    "model8.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model8.add(GRU(64))\n",
    "model8.add(Dropout(0.5))\n",
    "        \n",
    "        \n",
    "model8.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_13 (TimeDis (None, 30, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 30, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 30, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 30, 40, 40, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 30, 40, 40, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 30, 13, 13, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 30, 13, 13, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 30, 13, 13, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 30, 4, 4, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                209280    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 233,637\n",
      "Trainable params: 233,413\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model8.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.8567 - categorical_accuracy: 0.2911 Source path =  /home/datasets/Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59691, saving model to model_init_2021-10-2710_41_46.391063/model-00001-1.85671-0.29110-1.59691-0.23000.h5\n",
      "11/11 [==============================] - 228s 21s/step - loss: 1.8567 - categorical_accuracy: 0.2911 - val_loss: 1.5969 - val_categorical_accuracy: 0.2300\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5086 - categorical_accuracy: 0.3906 \n",
      "Epoch 00002: val_loss improved from 1.59691 to 1.53410, saving model to model_init_2021-10-2710_41_46.391063/model-00002-1.50860-0.39065-1.53410-0.34000.h5\n",
      "11/11 [==============================] - 217s 20s/step - loss: 1.5086 - categorical_accuracy: 0.3906 - val_loss: 1.5341 - val_categorical_accuracy: 0.3400\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2258 - categorical_accuracy: 0.5264 \n",
      "Epoch 00003: val_loss improved from 1.53410 to 1.48937, saving model to model_init_2021-10-2710_41_46.391063/model-00003-1.22575-0.52640-1.48937-0.36000.h5\n",
      "11/11 [==============================] - 222s 20s/step - loss: 1.2258 - categorical_accuracy: 0.5264 - val_loss: 1.4894 - val_categorical_accuracy: 0.3600\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0876 - categorical_accuracy: 0.5505 \n",
      "Epoch 00004: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 212s 19s/step - loss: 1.0876 - categorical_accuracy: 0.5505 - val_loss: 1.5024 - val_categorical_accuracy: 0.2800\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9620 - categorical_accuracy: 0.6259 \n",
      "Epoch 00005: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 221s 20s/step - loss: 0.9620 - categorical_accuracy: 0.6259 - val_loss: 1.5323 - val_categorical_accuracy: 0.2700\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8077 - categorical_accuracy: 0.6983 \n",
      "Epoch 00006: val_loss did not improve from 1.48937\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "11/11 [==============================] - 212s 19s/step - loss: 0.8077 - categorical_accuracy: 0.6983 - val_loss: 1.6170 - val_categorical_accuracy: 0.2300\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7148 - categorical_accuracy: 0.7421 \n",
      "Epoch 00007: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 220s 20s/step - loss: 0.7148 - categorical_accuracy: 0.7421 - val_loss: 1.6324 - val_categorical_accuracy: 0.2100\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6204 - categorical_accuracy: 0.7677 \n",
      "Epoch 00008: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 205s 19s/step - loss: 0.6204 - categorical_accuracy: 0.7677 - val_loss: 1.6799 - val_categorical_accuracy: 0.1800\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5400 - categorical_accuracy: 0.8190 \n",
      "Epoch 00009: val_loss did not improve from 1.48937\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "11/11 [==============================] - 203s 18s/step - loss: 0.5400 - categorical_accuracy: 0.8190 - val_loss: 1.6967 - val_categorical_accuracy: 0.1900\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5007 - categorical_accuracy: 0.8281 \n",
      "Epoch 00010: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 197s 18s/step - loss: 0.5007 - categorical_accuracy: 0.8281 - val_loss: 1.7393 - val_categorical_accuracy: 0.1900\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4430 - categorical_accuracy: 0.8658 \n",
      "Epoch 00011: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 205s 19s/step - loss: 0.4430 - categorical_accuracy: 0.8658 - val_loss: 1.8038 - val_categorical_accuracy: 0.1800\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4145 - categorical_accuracy: 0.8854 \n",
      "Epoch 00012: val_loss did not improve from 1.48937\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "11/11 [==============================] - 196s 18s/step - loss: 0.4145 - categorical_accuracy: 0.8854 - val_loss: 1.8431 - val_categorical_accuracy: 0.1900\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3713 - categorical_accuracy: 0.9005 \n",
      "Epoch 00013: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 211s 19s/step - loss: 0.3713 - categorical_accuracy: 0.9005 - val_loss: 1.8884 - val_categorical_accuracy: 0.1900\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3830 - categorical_accuracy: 0.8974 \n",
      "Epoch 00014: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 202s 18s/step - loss: 0.3830 - categorical_accuracy: 0.8974 - val_loss: 1.9236 - val_categorical_accuracy: 0.2000\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3615 - categorical_accuracy: 0.8914 \n",
      "Epoch 00015: val_loss did not improve from 1.48937\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "11/11 [==============================] - 205s 19s/step - loss: 0.3615 - categorical_accuracy: 0.8914 - val_loss: 1.9488 - val_categorical_accuracy: 0.1900\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3628 - categorical_accuracy: 0.9065 \n",
      "Epoch 00016: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 199s 18s/step - loss: 0.3628 - categorical_accuracy: 0.9065 - val_loss: 1.9252 - val_categorical_accuracy: 0.2000\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3603 - categorical_accuracy: 0.9020 \n",
      "Epoch 00017: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 204s 19s/step - loss: 0.3603 - categorical_accuracy: 0.9020 - val_loss: 1.9975 - val_categorical_accuracy: 0.1900\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3132 - categorical_accuracy: 0.9276 \n",
      "Epoch 00018: val_loss did not improve from 1.48937\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "11/11 [==============================] - 194s 18s/step - loss: 0.3132 - categorical_accuracy: 0.9276 - val_loss: 2.0872 - val_categorical_accuracy: 0.1700\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3310 - categorical_accuracy: 0.9216 \n",
      "Epoch 00019: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 200s 18s/step - loss: 0.3310 - categorical_accuracy: 0.9216 - val_loss: 2.0445 - val_categorical_accuracy: 0.2100\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3207 - categorical_accuracy: 0.9140 \n",
      "Epoch 00020: val_loss did not improve from 1.48937\n",
      "11/11 [==============================] - 198s 18s/step - loss: 0.3207 - categorical_accuracy: 0.9140 - val_loss: 2.1234 - val_categorical_accuracy: 0.2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f906aa8d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9 - Using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Use Transfer Learing ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "restnet_transfer = ResNet50(weights='imagenet', include_top=False,pooling='max')\n",
    "\n",
    "#Make weight of the transer model trainable false\n",
    "restnet_transfer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "model9 = Sequential()\n",
    "model9.add(TimeDistributed(restnet_transfer,input_shape=(30,120,120,3)))\n",
    "\n",
    "model9.add(GRU(64))\n",
    "model9.add(Dropout(0.5))\n",
    "\n",
    "model9.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_23 (TimeDis (None, 30, 2048)          23587712  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                405888    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 23,993,925\n",
      "Trainable params: 406,213\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model9.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model9.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.0402 - categorical_accuracy: 0.2006 Source path =  /home/datasets/Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.57314, saving model to model_init_2021-10-2710_41_46.391063/model-00001-2.04016-0.20060-1.57314-0.32000.h5\n",
      "11/11 [==============================] - 207s 19s/step - loss: 2.0402 - categorical_accuracy: 0.2006 - val_loss: 1.5731 - val_categorical_accuracy: 0.3200\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.7752 - categorical_accuracy: 0.2413 \n",
      "Epoch 00002: val_loss improved from 1.57314 to 1.46774, saving model to model_init_2021-10-2710_41_46.391063/model-00002-1.77522-0.24133-1.46774-0.49000.h5\n",
      "11/11 [==============================] - 201s 18s/step - loss: 1.7752 - categorical_accuracy: 0.2413 - val_loss: 1.4677 - val_categorical_accuracy: 0.4900\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6502 - categorical_accuracy: 0.2851 \n",
      "Epoch 00003: val_loss improved from 1.46774 to 1.45811, saving model to model_init_2021-10-2710_41_46.391063/model-00003-1.65020-0.28507-1.45811-0.34000.h5\n",
      "11/11 [==============================] - 208s 19s/step - loss: 1.6502 - categorical_accuracy: 0.2851 - val_loss: 1.4581 - val_categorical_accuracy: 0.3400\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6259 - categorical_accuracy: 0.2700 \n",
      "Epoch 00004: val_loss improved from 1.45811 to 1.41400, saving model to model_init_2021-10-2710_41_46.391063/model-00004-1.62590-0.26998-1.41400-0.43000.h5\n",
      "11/11 [==============================] - 205s 19s/step - loss: 1.6259 - categorical_accuracy: 0.2700 - val_loss: 1.4140 - val_categorical_accuracy: 0.4300\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5367 - categorical_accuracy: 0.3243 \n",
      "Epoch 00005: val_loss did not improve from 1.41400\n",
      "11/11 [==============================] - 213s 19s/step - loss: 1.5367 - categorical_accuracy: 0.3243 - val_loss: 1.4500 - val_categorical_accuracy: 0.3200\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.4738 - categorical_accuracy: 0.3725 \n",
      "Epoch 00006: val_loss improved from 1.41400 to 1.32673, saving model to model_init_2021-10-2710_41_46.391063/model-00006-1.47381-0.37255-1.32673-0.46000.h5\n",
      "11/11 [==============================] - 214s 19s/step - loss: 1.4738 - categorical_accuracy: 0.3725 - val_loss: 1.3267 - val_categorical_accuracy: 0.4600\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.4199 - categorical_accuracy: 0.3997 \n",
      "Epoch 00007: val_loss did not improve from 1.32673\n",
      "11/11 [==============================] - 232s 21s/step - loss: 1.4199 - categorical_accuracy: 0.3997 - val_loss: 1.3315 - val_categorical_accuracy: 0.4300\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3996 - categorical_accuracy: 0.3952 \n",
      "Epoch 00008: val_loss did not improve from 1.32673\n",
      "11/11 [==============================] - 221s 20s/step - loss: 1.3996 - categorical_accuracy: 0.3952 - val_loss: 1.3795 - val_categorical_accuracy: 0.4000\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3715 - categorical_accuracy: 0.4344 \n",
      "Epoch 00009: val_loss did not improve from 1.32673\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "11/11 [==============================] - 224s 20s/step - loss: 1.3715 - categorical_accuracy: 0.4344 - val_loss: 1.3520 - val_categorical_accuracy: 0.4100\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2907 - categorical_accuracy: 0.4736 \n",
      "Epoch 00010: val_loss improved from 1.32673 to 1.30064, saving model to model_init_2021-10-2710_41_46.391063/model-00010-1.29066-0.47360-1.30064-0.48000.h5\n",
      "11/11 [==============================] - 217s 20s/step - loss: 1.2907 - categorical_accuracy: 0.4736 - val_loss: 1.3006 - val_categorical_accuracy: 0.4800\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2401 - categorical_accuracy: 0.4917 \n",
      "Epoch 00011: val_loss improved from 1.30064 to 1.28409, saving model to model_init_2021-10-2710_41_46.391063/model-00011-1.24007-0.49170-1.28409-0.46000.h5\n",
      "11/11 [==============================] - 226s 21s/step - loss: 1.2401 - categorical_accuracy: 0.4917 - val_loss: 1.2841 - val_categorical_accuracy: 0.4600\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2210 - categorical_accuracy: 0.5204 \n",
      "Epoch 00012: val_loss improved from 1.28409 to 1.25486, saving model to model_init_2021-10-2710_41_46.391063/model-00012-1.22100-0.52036-1.25486-0.43000.h5\n",
      "11/11 [==============================] - 214s 19s/step - loss: 1.2210 - categorical_accuracy: 0.5204 - val_loss: 1.2549 - val_categorical_accuracy: 0.4300\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2054 - categorical_accuracy: 0.5294 \n",
      "Epoch 00013: val_loss did not improve from 1.25486\n",
      "11/11 [==============================] - 228s 21s/step - loss: 1.2054 - categorical_accuracy: 0.5294 - val_loss: 1.2731 - val_categorical_accuracy: 0.4600\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2060 - categorical_accuracy: 0.5234 \n",
      "Epoch 00014: val_loss improved from 1.25486 to 1.22621, saving model to model_init_2021-10-2710_41_46.391063/model-00014-1.20602-0.52338-1.22621-0.41000.h5\n",
      "11/11 [==============================] - 221s 20s/step - loss: 1.2060 - categorical_accuracy: 0.5234 - val_loss: 1.2262 - val_categorical_accuracy: 0.4100\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1731 - categorical_accuracy: 0.5294 \n",
      "Epoch 00015: val_loss did not improve from 1.22621\n",
      "11/11 [==============================] - 225s 20s/step - loss: 1.1731 - categorical_accuracy: 0.5294 - val_loss: 1.2704 - val_categorical_accuracy: 0.4500\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1624 - categorical_accuracy: 0.5656 \n",
      "Epoch 00016: val_loss improved from 1.22621 to 1.19867, saving model to model_init_2021-10-2710_41_46.391063/model-00016-1.16235-0.56561-1.19867-0.50000.h5\n",
      "11/11 [==============================] - 220s 20s/step - loss: 1.1624 - categorical_accuracy: 0.5656 - val_loss: 1.1987 - val_categorical_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1333 - categorical_accuracy: 0.5505 \n",
      "Epoch 00017: val_loss did not improve from 1.19867\n",
      "11/11 [==============================] - 215s 20s/step - loss: 1.1333 - categorical_accuracy: 0.5505 - val_loss: 1.2496 - val_categorical_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0954 - categorical_accuracy: 0.5656 \n",
      "Epoch 00018: val_loss did not improve from 1.19867\n",
      "11/11 [==============================] - 214s 19s/step - loss: 1.0954 - categorical_accuracy: 0.5656 - val_loss: 1.2355 - val_categorical_accuracy: 0.4600\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0961 - categorical_accuracy: 0.5656 \n",
      "Epoch 00019: val_loss did not improve from 1.19867\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "11/11 [==============================] - 218s 20s/step - loss: 1.0961 - categorical_accuracy: 0.5656 - val_loss: 1.2533 - val_categorical_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0291 - categorical_accuracy: 0.6214 \n",
      "Epoch 00020: val_loss did not improve from 1.19867\n",
      "11/11 [==============================] - 206s 19s/step - loss: 1.0291 - categorical_accuracy: 0.6214 - val_loss: 1.2664 - val_categorical_accuracy: 0.4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9df4186550>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 10 - Using MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "#Use Transfer Learning MobileNet\n",
    "from tensorflow.keras.applications import mobilenet\n",
    "\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "#Make weight of the transer model trainable false\n",
    "mobilenet_transfer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = Sequential()\n",
    "model10.add(TimeDistributed(mobilenet_transfer,input_shape=(30,120,120,3)))\n",
    "        \n",
    "model10.add(TimeDistributed(BatchNormalization()))\n",
    "model10.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model10.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model10.add(GRU(64))\n",
    "model10.add(Dropout(0.5))\n",
    "        \n",
    "model10.add(Dense(64,activation='relu'))\n",
    "model10.add(Dropout(0.5))\n",
    "        \n",
    "model10.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                209280    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,446,725\n",
      "Trainable params: 215,813\n",
      "Non-trainable params: 3,230,912\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model10.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-53-fc94ed450b6f>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.9778 - categorical_accuracy: 0.2428 Source path =  /home/datasets/Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54500, saving model to model_init_2021-10-2715_44_58.160471/model-00001-1.97778-0.24284-1.54500-0.24000.h5\n",
      "11/11 [==============================] - 203s 18s/step - loss: 1.9778 - categorical_accuracy: 0.2428 - val_loss: 1.5450 - val_categorical_accuracy: 0.2400\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6003 - categorical_accuracy: 0.3047 \n",
      "Epoch 00002: val_loss improved from 1.54500 to 1.46983, saving model to model_init_2021-10-2715_44_58.160471/model-00002-1.60028-0.30468-1.46983-0.44000.h5\n",
      "11/11 [==============================] - 200s 18s/step - loss: 1.6003 - categorical_accuracy: 0.3047 - val_loss: 1.4698 - val_categorical_accuracy: 0.4400\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.4038 - categorical_accuracy: 0.4148 \n",
      "Epoch 00003: val_loss improved from 1.46983 to 1.32971, saving model to model_init_2021-10-2715_44_58.160471/model-00003-1.40378-0.41478-1.32971-0.63000.h5\n",
      "11/11 [==============================] - 206s 19s/step - loss: 1.4038 - categorical_accuracy: 0.4148 - val_loss: 1.3297 - val_categorical_accuracy: 0.6300\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2961 - categorical_accuracy: 0.4811 \n",
      "Epoch 00004: val_loss improved from 1.32971 to 1.20451, saving model to model_init_2021-10-2715_44_58.160471/model-00004-1.29608-0.48115-1.20451-0.58000.h5\n",
      "11/11 [==============================] - 197s 18s/step - loss: 1.2961 - categorical_accuracy: 0.4811 - val_loss: 1.2045 - val_categorical_accuracy: 0.5800\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1928 - categorical_accuracy: 0.5173 \n",
      "Epoch 00005: val_loss improved from 1.20451 to 1.05602, saving model to model_init_2021-10-2715_44_58.160471/model-00005-1.19282-0.51735-1.05602-0.69000.h5\n",
      "11/11 [==============================] - 202s 18s/step - loss: 1.1928 - categorical_accuracy: 0.5173 - val_loss: 1.0560 - val_categorical_accuracy: 0.6900\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0677 - categorical_accuracy: 0.5973 \n",
      "Epoch 00006: val_loss improved from 1.05602 to 0.92028, saving model to model_init_2021-10-2715_44_58.160471/model-00006-1.06767-0.59729-0.92028-0.75000.h5\n",
      "11/11 [==============================] - 197s 18s/step - loss: 1.0677 - categorical_accuracy: 0.5973 - val_loss: 0.9203 - val_categorical_accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9460 - categorical_accuracy: 0.6591 \n",
      "Epoch 00007: val_loss improved from 0.92028 to 0.86649, saving model to model_init_2021-10-2715_44_58.160471/model-00007-0.94599-0.65913-0.86649-0.71000.h5\n",
      "11/11 [==============================] - 203s 18s/step - loss: 0.9460 - categorical_accuracy: 0.6591 - val_loss: 0.8665 - val_categorical_accuracy: 0.7100\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8405 - categorical_accuracy: 0.6682 \n",
      "Epoch 00008: val_loss did not improve from 0.86649\n",
      "11/11 [==============================] - 199s 18s/step - loss: 0.8405 - categorical_accuracy: 0.6682 - val_loss: 0.8738 - val_categorical_accuracy: 0.6600\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7485 - categorical_accuracy: 0.7315 \n",
      "Epoch 00009: val_loss improved from 0.86649 to 0.72625, saving model to model_init_2021-10-2715_44_58.160471/model-00009-0.74850-0.73152-0.72625-0.71000.h5\n",
      "11/11 [==============================] - 205s 19s/step - loss: 0.7485 - categorical_accuracy: 0.7315 - val_loss: 0.7263 - val_categorical_accuracy: 0.7100\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6960 - categorical_accuracy: 0.7587 \n",
      "Epoch 00010: val_loss improved from 0.72625 to 0.62505, saving model to model_init_2021-10-2715_44_58.160471/model-00010-0.69602-0.75867-0.62505-0.80000.h5\n",
      "11/11 [==============================] - 195s 18s/step - loss: 0.6960 - categorical_accuracy: 0.7587 - val_loss: 0.6250 - val_categorical_accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5697 - categorical_accuracy: 0.7903 \n",
      "Epoch 00011: val_loss did not improve from 0.62505\n",
      "11/11 [==============================] - 205s 19s/step - loss: 0.5697 - categorical_accuracy: 0.7903 - val_loss: 0.6641 - val_categorical_accuracy: 0.7400\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5077 - categorical_accuracy: 0.8220 \n",
      "Epoch 00012: val_loss did not improve from 0.62505\n",
      "11/11 [==============================] - 195s 18s/step - loss: 0.5077 - categorical_accuracy: 0.8220 - val_loss: 0.6284 - val_categorical_accuracy: 0.7800\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4022 - categorical_accuracy: 0.8854 \n",
      "Epoch 00013: val_loss did not improve from 0.62505\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "11/11 [==============================] - 205s 19s/step - loss: 0.4022 - categorical_accuracy: 0.8854 - val_loss: 0.6389 - val_categorical_accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3831 - categorical_accuracy: 0.8839 \n",
      "Epoch 00014: val_loss improved from 0.62505 to 0.52346, saving model to model_init_2021-10-2715_44_58.160471/model-00014-0.38308-0.88386-0.52346-0.79000.h5\n",
      "11/11 [==============================] - 198s 18s/step - loss: 0.3831 - categorical_accuracy: 0.8839 - val_loss: 0.5235 - val_categorical_accuracy: 0.7900\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3597 - categorical_accuracy: 0.8869 \n",
      "Epoch 00015: val_loss did not improve from 0.52346\n",
      "11/11 [==============================] - 204s 19s/step - loss: 0.3597 - categorical_accuracy: 0.8869 - val_loss: 0.6324 - val_categorical_accuracy: 0.7200\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3412 - categorical_accuracy: 0.8929 \n",
      "Epoch 00016: val_loss did not improve from 0.52346\n",
      "11/11 [==============================] - 195s 18s/step - loss: 0.3412 - categorical_accuracy: 0.8929 - val_loss: 0.5927 - val_categorical_accuracy: 0.7400\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2909 - categorical_accuracy: 0.9020 \n",
      "Epoch 00017: val_loss did not improve from 0.52346\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "11/11 [==============================] - 202s 18s/step - loss: 0.2909 - categorical_accuracy: 0.9020 - val_loss: 0.6183 - val_categorical_accuracy: 0.7300\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3119 - categorical_accuracy: 0.9035 \n",
      "Epoch 00018: val_loss improved from 0.52346 to 0.50407, saving model to model_init_2021-10-2715_44_58.160471/model-00018-0.31194-0.90347-0.50407-0.78000.h5\n",
      "11/11 [==============================] - 194s 18s/step - loss: 0.3119 - categorical_accuracy: 0.9035 - val_loss: 0.5041 - val_categorical_accuracy: 0.7800\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2724 - categorical_accuracy: 0.9201 \n",
      "Epoch 00019: val_loss did not improve from 0.50407\n",
      "11/11 [==============================] - 199s 18s/step - loss: 0.2724 - categorical_accuracy: 0.9201 - val_loss: 0.6153 - val_categorical_accuracy: 0.7200\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2942 - categorical_accuracy: 0.9170 \n",
      "Epoch 00020: val_loss did not improve from 0.50407\n",
      "11/11 [==============================] - 196s 18s/step - loss: 0.2942 - categorical_accuracy: 0.9170 - val_loss: 0.6151 - val_categorical_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92183d3710>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model is chosen as Model 6 (Conv3D) since it gives the best train and validation accuracy obtained at epoch 18 with 93.9% training accuracy and 89% validation accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
